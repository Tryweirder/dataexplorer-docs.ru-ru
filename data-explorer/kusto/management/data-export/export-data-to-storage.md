---
title: Экспорт данных в хранилище - Azure Data Explorer Документы Майкрософт
description: В этой статье описаны данные об экспорте для хранения в Azure Data Explorer.
services: data-explorer
author: orspod
ms.author: orspodek
ms.reviewer: rkarlin
ms.service: data-explorer
ms.topic: reference
ms.date: 03/12/2020
ms.openlocfilehash: 12800955d1680a280aa4772db86d8b71e44e7089
ms.sourcegitcommit: 47a002b7032a05ef67c4e5e12de7720062645e9e
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/15/2020
ms.locfileid: "81521579"
---
# <a name="export-data-to-storage"></a>Экспортные данные для хранения

Выполняет запрос и записывает первый набор результатов на внешнее хранилище, указанное [строкой подключения к хранилищу.](../../api/connection-strings/storage.md)

**Синтаксис**

`.export`-`async``compressed` `to` *OutputDataФорматта* 
 `(` *StorageConnectionString* `,` . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . `)` `with` `(` *НедвижимостьНаит* `=` *НедвижимостьСтоимость* `,` `)`Запрос `<|` *Query*

**Аргументы**

* `async`: Если указано, указывает, что команда работает в асинхронном режиме.
  Ниже приведены более подробные сведения о поведении в этом режиме.

* `compressed`: Если указано, артефакты накопительного хранилища сжимаются в виде `.gz` файлов. Смотрите `compressionType` для сжатия паркет файлы, как быстро. 

* *OutputDataFormat*: Указывает формат данных артефактов хранения, написанных командой. Поддерживаемые значения: `csv` `tsv`, `json`, `parquet`и .

* *StorageConnectionString*: Определяет одну или несколько [строк подключения к хранилищу,](../../api/connection-strings/storage.md) которые указывают, в какое хранилище писать данные. (Для масштабируемых записей может быть указано несколько строк подключения к хранилищу.) Каждая такая строка соединения должна указывать учетные данные, которые можно использовать при записи для хранения.
  Например, при записи в Хранилище Azure Blob, учетные данные могут быть ключом учетной записи хранилища или общим ключом доступа (SAS) с разрешениями читать, писать и перечислять капли.

> [!NOTE]
> Настоятельно рекомендуется экспортировать данные для хранения данных, которые расположены в том же регионе, что и сам кластер Кусто. Это включает в себя данные, которые экспортируются, чтобы они могли быть переданы в другой облачный сервис в других регионах. Записи должны быть сделаны локально, в то время как читает может происходить удаленно.

* *PropertyName*/*PropertyValue*: Нулевая или более необязательная экспортная недвижимость:

|Свойство        |Тип    |Описание                                                                                                                |
|----------------|--------|---------------------------------------------------------------------------------------------------------------------------|
|`sizeLimit`     |`long`  |Ограничение размера в байтах одного артефакта хранилища (до сжатия). Разрешенный диапазон составляет 100 МБ (по умолчанию) до 1 ГБ.|
|`includeHeaders`|`string`|`csv` / Для `tsv` вывода контролирует генерацию заголовков столбцов. Может быть `none` одним из (по умолчанию; `all` нет испускаемых заголовков), (испускают строку заголовка в каждый артефакт хранения), или `firstFile` (испускают заголовок в первый артефакт хранения только).|
|`fileExtension` |`string`|Указывает на "расширение" части артефакта хранения `.csv` `.tsv`(например, или ). Если сжатие используется, `.gz` будет также приложено.|
|`namePrefix`    |`string`|Указывает префикс для добавления к каждому генерируемому имени артефакта хранения. Случайная приставка будет использоваться, если не будет указана.       |
|`encoding`      |`string`|Указывает, как кодировать `UTF8NoBOM` текст: (по умолчанию) или `UTF8BOM`. |
|`compressionType`|`string`|Указывает тип сжатия для использования. Возможные значения: `gzip` или `snappy`. Значение по умолчанию — `gzip`. `snappy`может (по желанию) `parquet` использоваться для формата. |
|`distribution`   |`string`  |Подсказка`single`распределения `per_node` `per_shard`( , ). Если значение `single`равно, один поток будет писать в хранилище. В противном случае экспорт будет писать из всех узлов, исполняющих запрос параллельно. Смотрите [оценить плагин оператора](../../query/evaluateoperator.md). По умолчанию равен `per_shard`.
|`distributed`   |`bool`  |Отключить/включить распределенный экспорт. Установка на ложную `single` эквивалентна намеку на распределение. Значение по умолчанию — true.
|`persistDetails`|`bool`  |Указано, что команда должна `async` сохранять свои результаты (см. флаг). По умолчанию `true` в async выполняется, но может быть выключен, если абонент не требует результатов). По умолчанию в `false` синхронных исполнениях, но может быть включен в них, а также. |
|`parquetRowGroupSize`|`int`  |Актуально только тогда, когда формат данных является паркетным. Контролирует размер группы строк в экспортированных файлах. Размер группы строкпой по умолчанию составляет 100000 записей.|

**Результаты**

Команды возвращают таблицу, описывая сгенерированные артефакты хранения.
Каждая запись описывает один артефакт и включает в себя путь хранения к артефакту и сколько записей данных он хранит.

|путь|NumRecords|
|---|---|
|http://storage1.blob.core.windows.net/containerName/export_1_d08afcae2f044c1092b279412dcb571b.csv|10|
|http://storage1.blob.core.windows.net/containerName/export_2_454c0f1359e24795b6529da8a0101330.csv|15|

**Асинхронный режим**

Если `async` флаг указан, команда выполняется в асинхронном режиме.
В этом режиме команда немедленно возвращается с идентификатором операции, а экспорт данных продолжается в фоновом режиме до завершения. Идентификатор операции, возвращенный командой, может быть использован для отслеживания ее хода и, в конечном счете, его результатов с помощью следующих команд:

* [.show операции](../operations.md#show-operations): Отслеживайте прогресс.
* [детали операции .show](../operations.md#show-operation-details): Получите результаты завершения.

Например, после успешного завершения можно получить результаты с помощью:

```kusto
.show operation f008dc1e-2710-47d8-8d34-0d562f5f8615 details
```

**Примеры** 

В этом примере Kusto запускает запрос, а затем экспортирует первый набор записей, производимых запросом, на одну или несколько сжатых капли CSV.
Метки названия столбцов добавляются в качестве первого ряда для каждого капли.

```kusto 
.export
  async compressed
  to csv (
    h@"https://storage1.blob.core.windows.net/containerName;secretKey",
    h@"https://storage1.blob.core.windows.net/containerName2;secretKey"
  ) with (
    sizeLimit=100000,
    namePrefix=export,
    includeHeaders=all,
    encoding =UTF8NoBOM
  )
  <| myLogs | where id == "moshe" | limit 10000
```

**Известные проблемы**

*Ошибки хранения во время команды экспорта*

По умолчанию экспортная команда распределяется таким образом, что все [объемы,](../extents-overview.md) содержащие данные для экспорта записи в хранилище одновременно. При крупном экспорте, когда количество таких объемов велико, это может привести к высокой нагрузке на хранение, что приводит к реуселекции хранилищ или временным ошибкам хранения. В таких случаях рекомендуется попытаться увеличить количество учетных записей хранения, предоставленных экспортной команде (нагрузка будет распределена между счетами) и/или уменьшить параллел, установив подсказку `per_node` распределения (см. свойства команды). Полностью отключение дистрибутива также возможно, но это может существенно повлиять на производительность команды.
 