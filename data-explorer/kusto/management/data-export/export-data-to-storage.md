---
title: Экспорт данных в хранилище — Azure обозреватель данных | Документация Майкрософт
description: В этой статье описывается экспорт данных в хранилище в обозреватель данных Azure.
services: data-explorer
author: orspod
ms.author: orspodek
ms.reviewer: rkarlin
ms.service: data-explorer
ms.topic: reference
ms.date: 03/12/2020
ms.openlocfilehash: 6b76f7a3ce61a0530d885de29c1a85d170431bb9
ms.sourcegitcommit: 4507466bdcc7dd07e6e2a68c0707b6226adc25af
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/23/2020
ms.locfileid: "87106440"
---
# <a name="export-data-to-storage"></a>Экспорт данных в хранилище

Выполняет запрос и записывает первый результирующий набор во внешнее хранилище, указанное в [строке подключения к хранилищу](../../api/connection-strings/storage.md).

**Синтаксис**

`.export`[ `async` ] [ `compressed` ] `to` *Аутпутдатаформат* 
 `(` *StorageConnectionString* [ `,` ...] `)` [ `with` `(` *PropertyName* `=` *propertyvalue* [ `,` ...] `)` ] `<|` *Запрос*

**Аргументы**

* `async`: Если указано, указывает, что команда выполняется в асинхронном режиме.
  Дополнительные сведения о поведении в этом режиме см. ниже.

* `compressed`: Если этот параметр указан, артефакты хранилища выходных данных сжимаются как `.gz` файлы. См `compressionType` . раздел для сжатия файлов Parquet как привязки. 

* *Аутпутдатаформат*: указывает формат данных артефактов хранилища, записанных командой. Поддерживаются следующие значения: `csv` , `tsv` , `json` и `parquet` .

* *StorageConnectionString*: указывает одну или несколько [строк подключения к хранилищу](../../api/connection-strings/storage.md) , которые указывают, в какое хранилище следует записывать данные. (Для масштабируемых операций записи может быть задано более одной строки подключения к хранилищу.) Каждая такая строка подключения должна указывать учетные данные, используемые при записи в хранилище.
  Например, при записи в хранилище BLOB-объектов Azure учетные данные могут быть ключом учетной записи хранения или общим ключом доступа (SAS) с разрешениями на чтение, запись и получение списка больших двоичных объектов.

> [!NOTE]
> Настоятельно рекомендуется экспортировать данные в хранилище, размещенное в том же регионе, что и сам кластер Kusto. Сюда входят экспортируемые данные, которые можно передать в другую облачную службу в других регионах. Операции записи должны выполняться локально, в то время как операции чтения могут выполняться удаленно.

* *PropertyName* / *Propertyvalue*: ноль или несколько необязательных свойств экспорта:

|Свойство.        |Type    |Описание                                                                                                                |
|----------------|--------|---------------------------------------------------------------------------------------------------------------------------|
|`sizeLimit`     |`long`  |Предельный размер в байтах одного артефакта хранилища, записываемого (до сжатия). Допустимый диапазон — 100 МБ (по умолчанию).|
|`includeHeaders`|`string`|Для `csv` / `tsv` выходных данных управляет созданием заголовков столбцов. Может быть одним из значений `none` (по умолчанию, строки заголовка не `all` выдаются), (выдача строки заголовка в каждый артефакт хранилища) или `firstFile` (выдача строки заголовка в первый артефакт хранилища).|
|`fileExtension` |`string`|Указывает часть "Extension" артефакта хранилища (например, `.csv` или `.tsv` ). Если используется сжатие, `.gz` будет добавлен и.|
|`namePrefix`    |`string`|Указывает префикс, добавляемый к каждому созданному имени артефакта хранилища. Если значение left не указано, будет использоваться случайный префикс.       |
|`encoding`      |`string`|Указывает, как кодировать текст: `UTF8NoBOM` (по умолчанию) или `UTF8BOM` . |
|`compressionType`|`string`|Указывает используемый тип сжатия. Возможные значения: `gzip` или `snappy`. Значение по умолчанию — `gzip`. `snappy`можно (необязательно) использовать для `parquet` форматирования. |
|`distribution`   |`string`  |Указание распространения ( `single` , `per_node` , `per_shard` ). Если значение равно `single` , один поток будет выполнять запись в хранилище. В противном случае при экспорте будут записываться все узлы, которые выполняют запрос параллельно. См. раздел [оператор Evaluate подключаемого модуля](../../query/evaluateoperator.md). По умолчанию — `per_shard`.
|`distributed`   |`bool`  |Включение и отключение распределенного экспорта. Значение false эквивалентно указанию в `single` подсказке о распространении. Значение по умолчанию — true.
|`persistDetails`|`bool`  |Указывает, что команда должна сохранять свои результаты (см `async` . флаг). По умолчанию принимает значение `true` в асинхронных запусках, но может быть отключено, если вызывающий объект не требует результатов. Значение по умолчанию `false` — в синхронных выполнениях, но может быть включено и. |
|`parquetRowGroupSize`|`int`  |Имеет смысл, только если формат данных — Parquet. Управляет размером группы строк в экспортируемых файлах. Размер группы строк по умолчанию — 100000 записей.|

**Результаты**

Команды возвращают таблицу, описывающую созданные артефакты хранилища.
Каждая запись описывает один артефакт и включает путь хранилища к артефакту и количество записей данных, которые он содержит.

|Путь|нумрекордс|
|---|---|
|http://storage1.blob.core.windows.net/containerName/export_1_d08afcae2f044c1092b279412dcb571b.csv|10|
|http://storage1.blob.core.windows.net/containerName/export_2_454c0f1359e24795b6529da8a0101330.csv|15|

**Асинхронный режим**

Если `async` указан флаг, команда выполняется в асинхронном режиме.
В этом режиме команда сразу же возвращает идентификатор операции, а экспорт данных остается в фоновом режиме до завершения. ИДЕНТИФИКАТОР операции, возвращаемый командой, можно использовать для отслеживания хода выполнения и в конечном итоге с помощью следующих команд:

* [. показывать операции](../operations.md#show-operations): отслеживать ход выполнения.
* [. Отображение сведений об операции](../operations.md#show-operation-details): получение результатов выполнения.

Например, после успешного завершения можно получить результаты с помощью:

```kusto
.show operation f008dc1e-2710-47d8-8d34-0d562f5f8615 details
```

**Примеры** 

В этом примере Kusto выполняет запрос, а затем экспортирует первый набор записей, созданный запросом, в один или несколько сжатых больших двоичных объектов CSV.
Метки имен столбцов добавляются в качестве первой строки для каждого большого двоичного объекта.

```kusto 
.export
  async compressed
  to csv (
    h@"https://storage1.blob.core.windows.net/containerName;secretKey",
    h@"https://storage1.blob.core.windows.net/containerName2;secretKey"
  ) with (
    sizeLimit=100000,
    namePrefix=export,
    includeHeaders=all,
    encoding =UTF8NoBOM
  )
  <| myLogs | where id == "moshe" | limit 10000
```

#### <a name="known-issues"></a>Известные проблемы

*Ошибки хранилища при выполнении команды экспорта*

По умолчанию команда экспорта распространяется так, что все [экстенты](../extents-overview.md) , содержащие данные для экспорта, одновременно могут быть записаны в хранилище. При больших экспортах, когда число таких экстентов велико, это может привести к высокой нагрузке на хранилище, что приводит к регулированию хранилища или к ошибкам временного хранилища. В таких случаях рекомендуется попытаться увеличить количество учетных записей хранения, предоставленных команде Export (нагрузка будет распределена между учетными записями), и/или уменьшить параллелизм, задав для указания распределения значение `per_node` (см. раздел свойства команды). Кроме того, можно полностью отключить распространение, но это может значительно повлиять на производительность команды.
 